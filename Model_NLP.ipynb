{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNcuYupqB+8pV9TvRnG9cjR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vuminhphuong11/NLP/blob/main/Model_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvN_RpeuV8JZ",
        "outputId": "0b6830fd-5f8f-4ca9-ab4a-e365801e814e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pytorch)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jqKD5YJQYazJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8HfIMgHYRpR",
        "outputId": "fe8356f2-0174-4f56-a04d-b78ee1cd3d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install underthesea"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYWDN6_JZBAe",
        "outputId": "414bae82-a450-4c63-f738-dabcbb39f23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.32.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.6.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.2)\n",
            "Collecting underthesea-core==1.0.4 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.12.14)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.5.0)\n",
            "Downloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.11 underthesea-6.8.4 underthesea-core-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bs-g3rYBnmZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqKmVt8sZGhh",
        "outputId": "85f52de3-f810-46b5-b157-8ff3a5bbf32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vuminhphuong11/NLP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTU4P8Ubq8u9",
        "outputId": "49138623-f48b-4da5-a565-e1b31dfabed7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 113 (delta 47), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (113/113), 1.64 MiB | 11.26 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Đọc file JSON\n",
        "with open(\"/content/NLP/output_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Chuẩn bị danh sách các câu để huấn luyện Word2Vec\n",
        "sentences = [item[\"Comment\"] for item in data]\n",
        "\n",
        "# Huấn luyện Word2Vec\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Tạo embedding cho từng comment\n",
        "processed_data = []\n",
        "for item in data:\n",
        "    comment = item[\"Comment\"]\n",
        "    label = item[\"Label\"]\n",
        "    embedding = [model.wv[word] for word in comment if word in model.wv]  # Tạo embedding cho từng từ\n",
        "    processed_data.append({\n",
        "        \"comment\": comment,\n",
        "        \"label\": label,\n",
        "        \"embedding\": np.array(embedding)  # Chuyển thành NumPy array\n",
        "    })"
      ],
      "metadata": {
        "id": "AxhxpepUpu8E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chuẩn bị dữ liệu cho NumPy\n",
        "numpy_data = {\n",
        "    \"comments\": [record[\"comment\"] for record in processed_data],\n",
        "    \"labels\": [record[\"label\"] for record in processed_data],\n",
        "    \"embeddings\": [record[\"embedding\"] for record in processed_data]\n",
        "}\n",
        "\n",
        "# Lưu vào file .npy\n",
        "np.save(\"processed_data.npy\", numpy_data)"
      ],
      "metadata": {
        "id": "7O_wBPj3bWpC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chuyển embedding sang danh sách để JSON có thể lưu được\n",
        "json_data = [\n",
        "    {\n",
        "        \"comment\": record[\"comment\"],\n",
        "        \"label\": record[\"label\"],\n",
        "        \"embedding\": record[\"embedding\"].tolist()  # Chuyển từ NumPy array sang danh sách\n",
        "    }\n",
        "    for record in processed_data\n",
        "]\n",
        "\n",
        "# Lưu vào file .json\n",
        "with open(\"processed_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(json_data, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "XWBS81Kgbdl8"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}