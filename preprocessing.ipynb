{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #import thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re# thư viện để python làm việc với biểu thức chính quy\n",
    "import csv# thư viện để xử lí file .csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM, GRU, Input, GlobalMaxPooling1D, LayerNormalization, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from pyvi import ViTokenizer# thư viện để tokenize\n",
    "from pyvi import ViUtils "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # ghi dữ liệu sang một file mới\n",
    "### # loại bỏ stopword, kí tự, viết hoa\n",
    "### # kiểm tra xem có comment hoặc label nào bị trống không"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đọc dữ liệu từ file gốc và sau đó loại bỏ stopwords và lưu lại vào file ra\n",
    "input_file_path = 'train_data.csv'  # File CSV cần xử lý\n",
    "output_file_path = 'data.csv'       # File CSV đầu ra\n",
    "# Hàm đọc danh sách stopwords từ file\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = set(f.read().splitlines())  # Đọc từng dòng và lưu vào set\n",
    "    return stopwords\n",
    "# Lấy danh sách stopwords từ file\n",
    "stopwords = load_stopwords('vietnamese_stopwords.txt')\n",
    "# Hàm làm sạch văn bản\n",
    "def clean_text(text):\n",
    "    # Chuyển tất cả chữ về dạng viết thường\n",
    "    text = text.lower()\n",
    "    # Loại bỏ emoji, ký tự đặc biệt, chỉ giữ lại chữ cái và khoảng trắng\n",
    "    text = re.sub(r'[^\\w\\sáàạảãạăắằặẳẵâấầậẩẫéèẹẻẽêếềệểễíìịỉĩóòọỏõôốồộổỗơớờợởỡúùụủũưứừựửữýỳýỵỷỹđ]', '', text)\n",
    "    # Loại bỏ stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "# Đọc dữ liệu từ file CSV\n",
    "try:\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        data1 = list(reader)  # Đọc toàn bộ dữ liệu từ file CSV\n",
    "except FileNotFoundError:\n",
    "    print(f\"Không tìm thấy file: {input_file_path}\")\n",
    "    exit()\n",
    "except csv.Error:\n",
    "    print(f\"Lỗi khi đọc file CSV: {input_file_path}\")\n",
    "    exit()\n",
    "# Lọc và xử lý các mục comment, kiểm tra label và chỉ ghi lại những phần hợp lệ\n",
    "processed_data1 = []\n",
    "empty_rows = []  # Danh sách lưu dòng có dữ liệu trống\n",
    "for index, item in enumerate(data1):\n",
    "    comment = item.get('Comment', '').strip()\n",
    "    label = item.get('Label', '').strip()\n",
    "    # Kiểm tra dòng nào có dữ liệu bị trống\n",
    "    if not comment or not label:\n",
    "        empty_rows.append(index + 1)  # Lưu chỉ mục dòng (bắt đầu từ 1)\n",
    "        continue\n",
    "    # Xử lý comment\n",
    "    cleaned_comment = clean_text(comment)\n",
    "    processed_data1.append({'Comment': cleaned_comment, 'Label': label})\n",
    "# Báo các dòng có dữ liệu bị trống\n",
    "if empty_rows:\n",
    "    print(f\"Các dòng sau có dữ liệu trống ở cột 'Comment' hoặc 'Label': {empty_rows}\")\n",
    "# Ghi lại dữ liệu đã xử lý vào file CSV mới\n",
    "try:\n",
    "    with open(output_file_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['Comment', 'Label'])\n",
    "        writer.writeheader()  # Ghi tiêu đề cột\n",
    "        writer.writerows(processed_data1)  # Ghi dữ liệu\n",
    "    print(f\"Dữ liệu đã được ghi vào {output_file_path}\")\n",
    "except IOError:\n",
    "    print(f\"Không thể ghi dữ liệu vào file {output_file_path}\")*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #Lọc comment và label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data =pd.DataFrame({'Comment': data['Comment'],'Label': data['Label']})\n",
    "st_data = st_data.dropna()\n",
    "st_data = st_data.reset_index(drop=True)\n",
    "st_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Tạo một bản sao không có dấu, tokenization & lưu tạm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_comment = st_data['Comment'].values\n",
    "input_label = st_data['Label'].values\n",
    "label_dictionary ={'positive':2,'neutral':1,'negative':0}\n",
    "input_pre =[]\n",
    "label_accent =[]\n",
    "for idx, dt in enumerate(input_comment):\n",
    "    input_text_pre =list(tf.keras.preprocessing.text.text_to_word_sequence(dt))\n",
    "    input_text_pre = \" \".join(input_text_pre)\n",
    "    input_text_pre_no_accent = str(ViUtils.remove_accents(input_text_pre).decode(\"utf-8\"))# phần này là phần đã loại bỏ dấu\n",
    "    input_text_pre_accent =ViTokenizer.tokenize(input_text_pre)# tokenization cho văn bản có dấu\n",
    "    input_text_pre_no_accent = ViTokenizer.tokenize(input_text_pre_no_accent)# tokenization cho văn bản đã loại bỏ dấu\n",
    "    input_pre.append(input_text_pre_accent)\n",
    "    input_pre.append(input_text_pre_no_accent)\n",
    "    label_accent.append(input_label[idx])\n",
    "    label_accent.append(input_label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lấy thông số và kiểm tra tần xuất\n",
    "seq_len =[len(i.split())for i in input_pre[0:1000]]\n",
    "pd.Series(seq_len).hist(bins =10)\n",
    "plt.show()\n",
    "seq_len =[len(i.split())for i in input_pre[1000:2000]]\n",
    "pd.Series(seq_len).hist(bins =10)\n",
    "plt.show()\n",
    "seq_len =[len(i.split())for i in input_pre[2000:3000]]\n",
    "pd.Series(seq_len).hist(bins =10)\n",
    "plt.show()\n",
    "seq_len =[len(i.split())for i in input_pre[3000:4000]]\n",
    "pd.Series(seq_len).hist(bins =10)\n",
    "plt.show()\n",
    "seq_len =[len(i.split())for i in input_pre[4000:5000]]\n",
    "pd.Series(seq_len).hist(bins =10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data.csv' \n",
    "df = pd.read_csv(file_path)\n",
    "# Kiểm tra xem cột 'Comment' có tồn tại trong file không\n",
    "if 'Comment' in df.columns:\n",
    "    # Đếm số từ trong mỗi bình luận\n",
    "    df['word_count'] = df['Comment'].apply(lambda x: len(str(x).split()))  \n",
    "    # Tìm bình luận có số từ nhiều nhất\n",
    "    max_word_count_row = df.loc[df['word_count'].idxmax()]\n",
    "    print(f\"Bình luận có số từ nhiều nhất là ở dòng {max_word_count_row.name} với {max_word_count_row['word_count']} từ.\")\n",
    "    print(f\"Đoạn bình luận: {max_word_count_row['Comment']}\")\n",
    "else:\n",
    "    print(\"Không tìm thấy cột 'Comment' trong file CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chuyển đổi nhãn thành số và dạng one hot encoding\n",
    "label_idx =[label_dictionary[i]for i in label_accent]\n",
    "label_tf =tf.keras.utils.to_categorical(label_idx,num_classes=3)\n",
    "# Tạo một tokenizer cho việc tách từ trong văn bản\n",
    "tokenizer_data=Tokenizer(oov_token='<OOV>', split=' ')\n",
    "# huấn luyện\n",
    "tokenizer_data.fit_on_texts(input_pre)\n",
    "# chuyển văn bản thành các chuỗi số\n",
    "tokenizer_data_text=tokenizer_data.texts_to_sequences(input_pre)\n",
    "# Padding các chuỗi số, đảm bảo mỗi chuỗi có độ dài cố định (512 từ)\n",
    "vec_data = pad_sequences(tokenizer_data_text,padding='post',maxlen=512)\n",
    "# Lưu tokenizer đã huấn luyện vào file pickle để sử dụng sau này\n",
    "pickle.dump(tokenizer_data,open(\"tokenizer_data.pkl\", \"wb\"))\n",
    "print(\"comment data.shape \",vec_data.shape)\n",
    "data_voca_size =len(tokenizer_data.word_index)+1\n",
    "print(\"data_voca_size: \",data_voca_size)\n",
    "#chia tệp dữ liệu thành bộ train và bộ validation 8-2\n",
    "x_train, x_val,y_train,y_val= train_test_split(vec_data, label_tf,test_size=0.2,random_state=42)\n",
    "#chia tệp dữ liệu thành bộ train và bộ test 9-1\n",
    "x_train, x_test,y_train,y_test= train_test_split(vec_data, label_tf,test_size=0.1,random_state=42)\n",
    "print(\"training sample: \",len(x_train))\n",
    "print(\"validation sample: \",len(x_val))\n",
    "print(\"test sample: \",len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Build CNN +Bidirectional KSTM model for vietnamese language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():# hàm chạy chính cho model kstm và cnn\n",
    "    dropout_threshold =0.4\n",
    "    input_dim =data_voca_size\n",
    "    ouput_dim =32\n",
    "    input_length = 512\n",
    "    initializer= tf.keras.initializers.GlorotNormal()\n",
    "    \n",
    "    input_layer = Input(shape=(input_length,))\n",
    "    feature =Embedding(input_dim=input_dim, output_dim=ouput_dim,input_length=input_length, embeddings_initializer=\"GlorotNormal\")(input_layer)\n",
    "    cnn_feature =Conv1D(filters=32, kernel_size=3, padding='same',activation='relu')(feature)\n",
    "    cnn_feature =MaxPooling1D()(cnn_feature)\n",
    "    cnn_feature =Dropout(dropout_threshold)(cnn_feature)\n",
    "    cnn_feature = Conv1D(filters=32, kernel_size=3, padding='same',activation='relu')(cnn_feature)\n",
    "    cnn_feature =MaxPooling1D()(cnn_feature)\n",
    "    cnn_feature =LayerNormalization()(cnn_feature)\n",
    "    cnn_feature = Dropout(dropout_threshold)(cnn_feature)\n",
    "    #bidirectional kstm\n",
    "    bi_sktm_feature = Bidirectional(LSTM(units=32, dropout=dropout_threshold,return_sequences=True,kernel_initializer=initializer),merge_mode='concat')(feature)\n",
    "    bi_sktm_feature = MaxPooling1D()(bi_sktm_feature)\n",
    "    bi_sktm_feature = Bidirectional(GRU(units=32, dropout=dropout_threshold,return_sequences=True,kernel_initializer=initializer),merge_mode='concat')(bi_sktm_feature)\n",
    "    bi_sktm_feature = MaxPooling1D()(bi_sktm_feature)\n",
    "    bi_sktm_feature = LayerNormalization()(bi_sktm_feature)\n",
    "    combine_feature =tf.keras.layers.Concatenate()([cnn_feature,bi_sktm_feature])\n",
    "    combine_feature =GlobalMaxPooling1D()(combine_feature)\n",
    "    combine_feature =LayerNormalization()(combine_feature)\n",
    "    Classifier =Dense(90, activation='relu')(combine_feature)\n",
    "    Classifier =Dropout(0.2)(Classifier)\n",
    "    Classifier =Dense(70, activation='relu')(Classifier)\n",
    "    Classifier =Dropout(0.2)(Classifier)\n",
    "    Classifier =Dense(50, activation='relu')(Classifier)\n",
    "    Classifier =Dropout(0.2)(Classifier)\n",
    "    Classifier =Dense(30, activation='relu')(Classifier)\n",
    "    Classifier =Dropout(0.2)(Classifier)\n",
    "    Classifier =Dense(3, activation='softmax')(Classifier)\n",
    "    \n",
    "    model= tf.keras.Model(inputs = input_layer, outputs =Classifier)\n",
    "    return model\n",
    "model =generate_model()\n",
    "adam = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Define model checkPoint & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_model =tf.keras.callbacks.ModelCheckpoint('model.keras',monitor='val_loss')\n",
    "history =model.fit(x=x_train,y=y_train,validation_data=(x_val,y_val),epochs=15, batch_size=128,callbacks=[callback_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #model load on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model.keras\")\n",
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(raw_input, tokenizer):\n",
    "    input_text_pre =list(tf.keras.preprocessing.text.text_to_word_sequence(raw_input))\n",
    "    input_text_pre =\" \".join(input_text_pre)\n",
    "    input_text_pre_accent= ViTokenizer.tokenize(input_text_pre)\n",
    "    print(\"text preprocessed: \",input_text_pre_accent)\n",
    "    tokenizer_data_text=tokenizer.texts_to_sequences([input_text_pre_accent])\n",
    "    vec_data = pad_sequences(tokenizer_data_text,padding='post',maxlen =512)\n",
    "    return vec_data\n",
    "def inference_model(input_feature,model):\n",
    "    output=model(input_feature).numpy()[0]\n",
    "    result =output.argmax()\n",
    "    conf =float(output.max())\n",
    "    label_dictionary={'negative':0,'neutral':1,'positive':2}\n",
    "    label =list(label_dictionary.keys())\n",
    "    return label[int(result)],conf\n",
    "def prediction(raw_input, tokenizer,model):\n",
    "    input_model=preprocess_input(raw_input,tokenizer_data)\n",
    "    result,conf=inference_model(input_model,model)\n",
    "    return result, conf\n",
    "m_model= generate_model()\n",
    "m_model=load_model('model.keras')\n",
    "with open(r\"tokenizer_data.pkl\",\"rb\")as input_file:\n",
    "    my_tokenizer=pickle.load(input_file)\n",
    "print(prediction(\"phim không hay\",my_tokenizer,m_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    text=input()\n",
    "    if text ==\"end\":\n",
    "        break\n",
    "    else:\n",
    "        print(prediction(text,my_tokenizer,m_model)[0]+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
